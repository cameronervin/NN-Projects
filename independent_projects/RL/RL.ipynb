{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jko6-NnZ5rU0"
      },
      "source": [
        "# Reinforcement Learning - Cross Entropy to Deep Q\n",
        "## Background\n",
        "\n",
        "### Origins\n",
        "Reinforcement learning (RL) originated from the idea of learning by interacting with an environment, using trial and error to maximize rewards. It combines elements from psychology, specifically the concept of behaviorist theories of learning, with computational approaches found in dynamic programming and optimal control theory. Early works in the 1950s by researchers like Donald Hebb introduced ideas about how neural mechanisms might reinforce certain behaviors. In the 1980s, RL gained more formal structure through work by Richard Sutton and Andrew Barto, who introduced key algorithms like temporal difference learning, integrating insights from both artificial intelligence and psychology.\n",
        "\n",
        "### What is Reinforcement Learning?\n",
        "RL is a branch of machine learning where an agent learns to make decisions by interacting with an environment. Unlike supervised learning, where models learn from labeled examples, in RL, an agent learns from the consequences of its actions through trial and error. The agent receives rewards or penalties based on its actions and uses this feedback to learn a policy or strategy that maximizes the cumulative reward over time. This process involves assessing different states of the environment, taking actions, and adjusting strategies based on outcomes. RL is particularly effective in scenarios where explicit correct actions are not known and is widely used in areas like robotics, gaming, and autonomous systems.\n",
        "\n",
        "![RL_basic_diagram.png](https://github.com/cameronervin/NN-Projects/blob/main/independent_projects/RL/images/RL_basic_diagram.png?raw=true)\n",
        "\n",
        "### Cross Entropy\n",
        "**What is Cross Entropy?**\n",
        "Cross entropy is a method used in reinforcement learning that relies on a simple yet effective trial-and-error approach to find good policies. It involves these key steps:\n",
        "\n",
        "- Sampling: Generate a set of potential solutions or policies randomly.\n",
        "- Evaluation: Assess these solutions by simulating how they perform in the environment, usually by measuring the total rewards they accumulate.\n",
        "- Selection: Identify the best-performing solutions from the sample, often choosing a top percentage (e.g., the best 20%).\n",
        "- Learning: Use the characteristics of these top solutions to guide the generation of new solutions. This often involves statistically analyzing the best solutions to update the probabilities of choosing certain actions in the future.\n",
        "- Iteration: Repeat the process with the new set of solutions, progressively refining the solutions with each cycle.\n",
        "\n",
        "**Why is Cross Entropy a Stepping Stone to More Complex Algorithms?**\n",
        "- Foundation in Probabilistic Exploration: The cross entropy method introduces the fundamental concept of probabilistic exploration and exploitation. It teaches how to balance between trying out new solutions and refining known good ones. This balance is central to many more advanced reinforcement learning algorithms.\n",
        "- Simplicity and Effectiveness: Because of its straightforward approach, cross entropy is easy to implement and can quickly produce good results in complex problem spaces with discrete actions. This makes it an ideal starting point for understanding how to approach learning and optimization in environments where direct calculation of the best action is not feasible.\n",
        "- Building Intuition for Optimization: Cross entropy helps build intuition about how stochastic processes can be used to navigate and optimize decision-making problems. This is invaluable when progressing to algorithms that require more nuanced understanding of probability distributions and their optimization, like those found in policy gradient methods or other advanced deep learning techniques.\n",
        "- Adaptability: The method teaches the critical skill of adaptively modifying solution strategies based on performance feedback. This adaptability is crucial for more complex learning environments and algorithms that deal with continuous action spaces or those that must adapt to changing conditions over time.\n",
        "\n",
        "In essence, the cross entropy method serves as a practical introduction to the concepts of policy search and optimization in reinforcement learning. It paves the way for understanding and developing more complex algorithms by providing a clear example of iterative improvement based on probabilistic exploration of the solution space. This groundwork is essential for tackling more advanced topics in reinforcement learning, where the environments and decision-making tasks become significantly more complex.\n",
        "\n",
        "### Value Iteration\n",
        "**What is Value Iteration?**\n",
        "\n",
        "Value iteration is a method in reinforcement learning used to find the optimal policy by systematically updating the values assigned to each state in a deterministic environment. This technique involves:\n",
        "\n",
        "- State Values: Calculating the value of each state based on the rewards expected to be obtained from them, accounting for all possible future states.\n",
        "- Iterative Updates: Repeatedly updating the values of each state using the Bellman equation until the values converge to a stable set that does not change significantly with further updates.\n",
        "- Bellman Equation: This fundamental formula in value iteration updates the value of each state by considering the maximum possible rewards that can be earned by taking the best action from that state.\n",
        "\n",
        "**Transition from Cross Entropy to Value Iteration**\n",
        "Cross entropy and value iteration are distinct approaches within reinforcement learning, each suited to different types of problems or stages of learning:\n",
        "\n",
        "- From Sampling to Deterministic Updates: The cross entropy method is a heuristic algorithm that relies on sampling and probabilistically selecting the best-performing solutions to guide the search for an optimal policy. It’s particularly useful in environments with large, discrete action spaces and is good for finding a policy that performs reasonably well within a complex or poorly understood environment. However, it does not guarantee finding the optimal policy.\n",
        "- Need for Precision and Optimality: Value iteration, on the other hand, is a more precise approach that iteratively calculates the exact value of each state until it converges to the optimal values. This method ensures that the resulting policy is truly optimal by systematically exploring the value of every possible state-action pair according to a predefined model of the environment.\n",
        "\n",
        "**Why the Transition is Necessary**\n",
        "\n",
        "- Scalability and Efficiency: While the cross entropy method can quickly narrow down a large space of possible policies to those that are more effective, it may not efficiently handle the fine-tuning needed to reach the optimal policy in environments where an accurate state-value relationship is crucial. Value iteration fills this gap by providing precise updates based on a full model of the environment.\n",
        "- Model-Based Learning: Transitioning to value iteration is necessary when a precise and optimal policy is required, especially in environments where the dynamics are well understood and can be modeled accurately. Value iteration leverages this full knowledge to calculate the optimal strategy rigorously.\n",
        "- Different Goals: Cross entropy is about finding a good enough policy across possibly very complex or high-dimensional spaces through sampling and selection, making it more explorative. Value iteration, in contrast, is about refining to the best possible policy through exhaustive and repeated calculation, making it more exhaustive and definitive.\n",
        "\n",
        "In summary, transitioning from cross entropy to value iteration is often driven by the need for a more precise and assured approach to finding the optimal policy, especially in environments where the actions’ effects and the rewards structure are completely known and can be modeled deterministically.\n",
        "\n",
        "### Value Iteration with a Q Function\n",
        "**What is Value Iteration with a Q Function?**\n",
        "\n",
        "Value iteration with a Q function, also known as Q-value iteration, is a method in reinforcement learning that extends traditional value iteration to directly calculate the value of taking specific actions in specific states. Unlike traditional value iteration, which focuses solely on the values of states, Q-value iteration computes and updates a Q-value for each state-action pair. These Q-values represent the expected total reward for taking a certain action in a certain state and then following the optimal policy.\n",
        "\n",
        "- Q-Values: Each state-action pair has an associated Q-value, which is an estimate of the total expected reward starting from that state, taking that action, and thereafter following the best possible strategy.\n",
        "- Updates: The Q-values are updated using a formula that considers the immediate reward plus the discounted maximum Q-value of the next state, reflecting the best future decision.\n",
        "\n",
        "**Transition from Value Iteration to Value Iteration with a Q Function**\n",
        "The transition from traditional value iteration to Q-value iteration involves a fundamental shift in the granularity and flexibility of the reinforcement learning approach:\n",
        "\n",
        "- Granularity: Traditional value iteration computes a value for each state based on the best possible future values, which inherently assumes an optimal decision will be made at each step. Q-value iteration, however, separately evaluates every possible action within each state, providing a more detailed and actionable set of data that directly informs the decision-making process.\n",
        "- Action-Specific Policy Making: In traditional value iteration, the policy (or best action per state) is derived indirectly by selecting the action that maximizes the state’s value after the values have been updated. In Q-value iteration, the policy can be directly obtained by selecting the action with the highest Q-value in each state. This direct approach simplifies policy extraction and can be more efficient in practice.\n",
        "- Flexibility and Robustness: Q-value iteration allows for more robust policy evaluation and improvement since it separately accounts for the value of each action in each state. This distinction is particularly important in environments where different actions have very diverse consequences, and the optimal action may vary significantly depending on subtle differences in state conditions.\n",
        "\n",
        "**Why the Change is Necessary**\n",
        "- Complex Environments: As environments increase in complexity and the impact of specific actions becomes more significant, having a detailed understanding of the value of each specific action (rather than just the value of being in a state) becomes crucial. This detailed understanding helps in better handling complex decision-making scenarios where the optimal action might vary dramatically based on the state’s nuances.\n",
        "- Improved Decision Making: By explicitly calculating and updating the value of state-action pairs, Q-value iteration provides clearer guidance for real-time decision making, allowing agents to make more informed choices that are based on directly relevant data.\n",
        "\n",
        "The shift to Q-value iteration represents a natural progression in developing more sophisticated and practical algorithms that can better navigate the complexities of real-world environments by providing more detailed and directly actionable insights.\n",
        "\n",
        "### Tabular Q Learning\n",
        "**What is Tabular Q-Learning?**\n",
        "\n",
        "Tabular Q-learning is a model-free reinforcement learning technique where an agent learns to make decisions by directly interacting with an environment, without needing a model of the environment's dynamics. The method involves maintaining a table (Q-table) that contains Q-values for each combination of state and action. These Q-values estimate the total expected reward for taking an action in a given state and then following the optimal policy thereafter.\n",
        "\n",
        "- Q-Table: A grid-like structure where each cell represents a state-action pair, with values updated based on the rewards received and the best future rewards expected.\n",
        "- Updates: The Q-values are updated using the experiences gathered from the environment (state, action, reward, next state) using a specific formula that accounts for immediate rewards and discounted future rewards.\n",
        "- Learning and Decision Making: The agent learns by continuously updating this table as it explores the environment, and it makes decisions based on the highest Q-value in the current state's row of the table.\n",
        "\n",
        "**Transition from Value Iteration with a Q Function to Tabular Q-Learning**\n",
        "Value iteration with a Q function, often known as Q-value iteration, and tabular Q-learning both involve learning the best actions to take in various states by updating Q-values. However, the transition from one to the other involves a shift in methodology and underlying assumptions:\n",
        "\n",
        "- Model Dependency: Q-value iteration typically requires a complete model of the environment, including knowledge of all possible states, actions, transitions, and rewards. This makes it a model-based approach. Tabular Q-learning, on the other hand, does not require prior knowledge of the environment's dynamics and learns directly from interactions, making it model-free.\n",
        "- Continuous vs. Discrete Updates: Q-value iteration performs updates in a batch or full sweep manner, updating the Q-values for all state-action pairs based on a presumed complete knowledge of the environment. Tabular Q-learning updates Q-values incrementally based on individual experiences (state, action, reward, next state), which it collects one at a time as the agent interacts with the environment.\n",
        "\n",
        "**Why the Change is Necessary**\n",
        "- Practicality in Unknown Environments: In many real-world applications, the exact details of the environment may not be fully known in advance. Tabular Q-learning's ability to learn without a complete model makes it more practical and widely applicable.\n",
        "- Adaptability: Since tabular Q-learning updates its Q-values based on actual interactions with the environment, it can adapt to changes in the environment's dynamics, which would not be immediately possible in a model-based approach like Q-value iteration.\n",
        "- Efficiency: For environments where the state and action spaces are large but manageable, tabular Q-learning can be more efficient and easier to implement than trying to maintain and compute a full model as required in Q-value iteration.\n",
        "\n",
        "In summary, the transition to tabular Q-learning offers more flexibility and adaptability in dynamic or unknown environments, making it a suitable approach for practical reinforcement learning applications where model assumptions may not hold.\n",
        "\n",
        "![tabular.png](https://github.com/cameronervin/NN-Projects/blob/main/independent_projects/RL/images/tabular.png?raw=true)\n",
        "\n",
        "### Deep Q Learning\n",
        "**What is Deep Q-Learning?**\n",
        "\n",
        "Deep Q-learning is an advanced form of Q-learning that uses neural networks, referred to as \"deep Q-networks\" (DQN), to estimate Q-values instead of storing them in a table. Here’s how it works:\n",
        "\n",
        "- Neural Network as a Function Approximator: The deep Q-network learns to predict Q-values (the expected rewards for taking actions in given states) directly from the states themselves. This allows it to handle environments with large or continuous state spaces where tabular methods would be impractical.\n",
        "- Input and Output: The input to the network is the state of the environment, and the output is the Q-value for each possible action from that state.\n",
        "- Training: The network is trained by adjusting its weights to minimize the difference between the predicted Q-value and the target Q-value, which is calculated using the reward received from the environment plus the discounted highest Q-value for the next state, similar to tabular Q-learning. Transition from Tabular Q-Learning to Deep Q-Learning\n",
        "\n",
        "**The transition from tabular Q-learning to deep Q-learning addresses several challenges:**\n",
        "- Scalability: Tabular Q-learning becomes unmanageable as the number of states and actions increases because the Q-table grows too large. Deep Q-learning uses a neural network to approximate Q-values, significantly reducing the need for memory and enabling scalability to larger problems.\n",
        "- Handling Continuous Spaces: Tabular methods are not feasible for environments with continuous state or action spaces because the table would have to be infinitely large. Deep Q-learning, by using function approximation, can generalize across similar states, making it suitable for continuous or very complex environments.\n",
        "- Generalization: Neural networks can learn to generalize from seen states to unseen states by finding patterns in the input data, something tabular methods cannot do. This makes deep Q-learning more robust and versatile.\n",
        "\n",
        "**Why the Change is Necessary**\n",
        "- Efficiency and Feasibility: For practical applications involving real-world problems, such as robotics or video games with rich and dynamic environments, tabular methods are often not feasible due to their high memory and computation costs. Deep Q-learning offers a viable alternative that can handle complexity and scale more effectively.\n",
        "- Improved Performance: Deep Q-learning can potentially lead to better performance through its ability to generalize and learn from a broader range of experiences than is possible with a strictly tabular approach. \n",
        "\n",
        "In essence, deep Q-learning represents a significant advancement in the capability of reinforcement learning to tackle more complex and varied problems by leveraging the power of deep neural networks for function approximation.\n",
        "\n",
        "![deepQ.png](https://github.com/cameronervin/NN-Projects/blob/main/independent_projects/RL/images/deepQ.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pZbKXPO5rU4"
      },
      "source": [
        "## Implementing Deep Q Learning\n",
        "\n",
        "Now, to implement Deep Q Learning with the cart-pole problem. \n",
        "\n",
        "#### Cart-Pole:\n",
        "The Cart Pole game is a classic reinforcement learning problem where the objective is to balance a pole attached by a pivot to a cart that moves along a track. The cart can move left or right on the track, and the player (or an AI agent) must choose these movements in such a way that the pole, which starts upright, does not fall over. The game becomes progressively more challenging as the pole begins to tilt, requiring precise and timely adjustments of the cart's position to keep the pole balanced. The goal is to keep the pole upright for as long as possible. The agent earns a reward for each second the pole stays upright.\n",
        "\n",
        "### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKg9SPsCrUpH",
        "outputId": "366ae552-3609-44b4-c7b9-c5ee300e7a3a"
      },
      "outputs": [],
      "source": [
        "!pip install -q gymnasium[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ktLO_qCprtmZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import collections\n",
        "from collections import namedtuple\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import gymnasium as gym\n",
        "from IPython.display import Video\n",
        "import imageio\n",
        "import cv2\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "torch.manual_seed(64)\n",
        "np.random.seed(64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I8gUhsfj2gyh"
      },
      "outputs": [],
      "source": [
        "# create CartPole environment\n",
        "env = gym.make('CartPole-v0', render_mode='rgb_array')\n",
        "state, _ = env.reset()\n",
        "frames = []  # list to store frames for video\n",
        "\n",
        "# run the environment for 20 steps\n",
        "for i in range(20):\n",
        "    frames.append(env.render())  # append current frame to the list\n",
        "\n",
        "    # choose a random action from the action space\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # take the chosen action and observe the next state, reward, and termination status\n",
        "    state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "    # if the episode is terminated or truncated, reset the environment\n",
        "    if terminated or truncated:\n",
        "        state, _ = env.reset()\n",
        "\n",
        "env.close()  # close the environment after exploration\n",
        "\n",
        "# create a video from frames\n",
        "out = cv2.VideoWriter('/cartpole_simulation_random.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 15, (frames[0].shape[1], frames[0].shape[0]))\n",
        "\n",
        "for frame in frames:\n",
        "    out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))  # convert RGB to BGR for OpenCV\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "l9YUVXlW2lLb",
        "outputId": "06b63caf-0cba-4a66-ed34-9629427c6dfc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<video controls  width=\"608\"  height=\"400\">\n",
              " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ],
            "text/plain": [
              "<IPython.core.display.Video object>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "video_path = '/cartpole_simulation_random.mp4'\n",
        "imageio.mimsave(video_path, frames, fps=30, macro_block_size=1)\n",
        "\n",
        "Video(video_path, embed=True, width=608, height=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z4d-A4Diwkqd"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    class FullyConnectedModel(nn.Module):\n",
        "        def __init__(self, input_size, output_size):\n",
        "            super(QNetwork.FullyConnectedModel, self).__init__()\n",
        "\n",
        "            # define layers with relu activation\n",
        "            self.linear1 = nn.Linear(input_size, 16)\n",
        "            self.activation1 = nn.ReLU()\n",
        "            self.linear2 = nn.Linear(16, 16)\n",
        "            self.activation2 = nn.ReLU()\n",
        "            self.linear3 = nn.Linear(16, 16)\n",
        "            self.activation3 = nn.ReLU()\n",
        "\n",
        "            # output layer without activation function\n",
        "            self.output_layer = nn.Linear(16, output_size)\n",
        "\n",
        "            # initialization using xavier uniform (a popular technique for initializing weights in nns)\n",
        "            nn.init.xavier_uniform_(self.linear1.weight)\n",
        "            nn.init.xavier_uniform_(self.linear2.weight)\n",
        "            nn.init.xavier_uniform_(self.linear3.weight)\n",
        "            nn.init.xavier_uniform_(self.output_layer.weight)\n",
        "\n",
        "        def forward(self, inputs):\n",
        "            # forward pass through the layers\n",
        "            x = self.activation1(self.linear1(inputs))\n",
        "            x = self.activation2(self.linear2(x))\n",
        "            x = self.activation3(self.linear3(x))\n",
        "            x = self.output_layer(x)\n",
        "            return x\n",
        "\n",
        "    def __init__(self, env, lr, logdir=None):\n",
        "        super(QNetwork, self).__init__()\n",
        "        # define q-network with specified architecture\n",
        "        self.net = QNetwork.FullyConnectedModel(4, 2)\n",
        "        self.env = env\n",
        "        self.lr = lr\n",
        "        self.logdir = logdir\n",
        "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
        "\n",
        "    def load_model(self, model_file):\n",
        "        # load pre-trained model from a file\n",
        "        return self.net.load_state_dict(torch.load(model_file))\n",
        "\n",
        "    def load_model_weights(self, weight_file):\n",
        "        # load pre-trained model weights from a file\n",
        "        return self.net.load_state_dict(torch.load(weight_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vZCi0qoawobT"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, env, memory_size=50000, burn_in=10000):\n",
        "        # initializes the replay memory, which stores transitions recorded from the agent taking actions in the environment.\n",
        "        self.memory_size = memory_size\n",
        "        self.burn_in = burn_in\n",
        "        self.memory = collections.deque([], maxlen=memory_size)\n",
        "        self.env = env\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        # returns a batch of randomly sampled transitions to be used for training the model.\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def append(self, transition):\n",
        "        # appends a transition to the replay memory.\n",
        "        self.memory.append(transition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YFeX_lRrxD9G"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class DQN_Agent:\n",
        "\n",
        "    def __init__(self, environment_name, lr=5e-4, render=False):\n",
        "        # initialize the DQN Agent\n",
        "        self.env = gym.make(environment_name)\n",
        "        self.lr = lr\n",
        "        self.policy_net = QNetwork(self.env, self.lr)\n",
        "        self.target_net = QNetwork(self.env, self.lr)\n",
        "        self.target_net.net.load_state_dict(self.policy_net.net.state_dict())  # copy the weight of the policy network\n",
        "        self.rm = ReplayMemory(self.env)\n",
        "        self.burn_in_memory()\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.99\n",
        "        self.c = 0\n",
        "\n",
        "    def burn_in_memory(self):\n",
        "        # initialize replay memory with a burn-in number of episodes/transitions.\n",
        "        cnt = 0\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        state, _ = self.env.reset()\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # iterate until we store \"burn_in\" buffer\n",
        "        while cnt < self.rm.burn_in:\n",
        "            # reset environment if terminated or truncated\n",
        "            if terminated or truncated:\n",
        "                state, _ = self.env.reset()\n",
        "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "            # randomly select an action (left or right) and take a step\n",
        "            action = torch.tensor(random.sample([0, 1], 1)[0]).reshape(1, 1)\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
        "            reward = torch.tensor([reward])\n",
        "            if terminated:\n",
        "                next_state = None\n",
        "            else:\n",
        "                next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "            # store new experience into memory\n",
        "            transition = Transition(state, action, next_state, reward)\n",
        "            self.rm.memory.append(transition)\n",
        "            state = next_state\n",
        "            cnt += 1\n",
        "\n",
        "    def epsilon_greedy_policy(self, q_values, epsilon=0.05):\n",
        "        # implement an epsilon-greedy policy.\n",
        "        p = random.random()\n",
        "        if p > epsilon:\n",
        "            with torch.no_grad():\n",
        "                return self.greedy_policy(q_values)\n",
        "        else:\n",
        "            return torch.tensor([[self.env.action_space.sample()]], dtype=torch.long)\n",
        "\n",
        "    def greedy_policy(self, q_values):\n",
        "        # implement a greedy policy for test time.\n",
        "        return torch.argmax(q_values)\n",
        "\n",
        "    def train(self):\n",
        "        # train the Q-network using Deep Q-learning.\n",
        "        state, _ = self.env.reset()\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        # loop until reaching the termination state\n",
        "        while not (terminated or truncated):\n",
        "            with torch.no_grad():\n",
        "                q_values = self.policy_net.net(state)\n",
        "\n",
        "            # decide the next action with epsilon greedy strategy\n",
        "            action = self.epsilon_greedy_policy(q_values).reshape(1, 1)\n",
        "\n",
        "            # take action and observe reward and next state\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
        "            reward = torch.tensor([reward])\n",
        "            if terminated:\n",
        "                next_state = None\n",
        "            else:\n",
        "                next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "            # store the new experience\n",
        "            transition = Transition(state, action, next_state, reward)\n",
        "            self.rm.memory.append(transition)\n",
        "\n",
        "            # move to the next state\n",
        "            state = next_state\n",
        "\n",
        "            # sample minibatch with size N from memory\n",
        "            transitions = self.rm.sample_batch(self.batch_size)\n",
        "            batch = Transition(*zip(*transitions))\n",
        "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
        "            non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "            state_batch = torch.cat(batch.state)\n",
        "            action_batch = torch.cat(batch.action)\n",
        "            reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "            # get current and next state values\n",
        "            state_action_values = self.policy_net.net(state_batch).gather(1, action_batch) # extract values corresponding to the actions Q(S_t, A_t)\n",
        "            next_state_values = torch.zeros(self.batch_size)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # no next_state_value update if an episode is terminated (next_satate = None)\n",
        "                # only update the non-termination state values (Ref: https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/)\n",
        "                next_state_values[non_final_mask] = self.target_net.net(non_final_next_states).max(1)[0] # extract max value\n",
        "\n",
        "            # update the model\n",
        "            expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
        "            criterion = torch.nn.MSELoss()\n",
        "            loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "            self.policy_net.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.policy_net.optimizer.step()\n",
        "\n",
        "            # update the target Q-network in each 50 steps\n",
        "            self.c += 1\n",
        "            if self.c % 50 == 0:\n",
        "                self.target_net.net.load_state_dict(self.policy_net.net.state_dict())\n",
        "\n",
        "    def test(self, model_file=None):\n",
        "        # evaluates the performance of the agent over 20 episodes.\n",
        "\n",
        "        max_t = 1000\n",
        "        state, _ = self.env.reset()\n",
        "        rewards = []\n",
        "\n",
        "        for t in range(max_t):\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.policy_net.net(state)\n",
        "            action = self.greedy_policy(q_values)\n",
        "            state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
        "            rewards.append(reward)\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        return np.sum(rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XIfC5pmAz08R",
        "outputId": "fc5bc901-a385-4da6-8231-88d1669480fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0\n",
            "The test reward for episode 0 is 9.5 with a standard deviation of 0.806225774829855.\n",
            "Episode: 10\n",
            "The test reward for episode 10 is 9.55 with a standard deviation of 0.6689544080129826.\n",
            "Episode: 20\n",
            "The test reward for episode 20 is 9.25 with a standard deviation of 1.0428326807307104.\n",
            "Episode: 30\n",
            "The test reward for episode 30 is 9.55 with a standard deviation of 0.49749371855331004.\n",
            "Episode: 40\n",
            "The test reward for episode 40 is 9.5 with a standard deviation of 0.5916079783099616.\n",
            "Episode: 50\n",
            "The test reward for episode 50 is 9.25 with a standard deviation of 0.698212002188447.\n",
            "Episode: 60\n",
            "The test reward for episode 60 is 9.3 with a standard deviation of 0.7810249675906654.\n",
            "Episode: 70\n",
            "The test reward for episode 70 is 9.2 with a standard deviation of 0.7483314773547882.\n",
            "Episode: 80\n",
            "The test reward for episode 80 is 9.35 with a standard deviation of 0.7262919523166976.\n",
            "Episode: 90\n",
            "The test reward for episode 90 is 9.4 with a standard deviation of 0.7348469228349535.\n",
            "Episode: 100\n",
            "The test reward for episode 100 is 9.35 with a standard deviation of 0.7921489758877431.\n",
            "Episode: 110\n",
            "The test reward for episode 110 is 9.45 with a standard deviation of 0.739932429347437.\n",
            "Episode: 120\n",
            "The test reward for episode 120 is 9.3 with a standard deviation of 0.9.\n",
            "Episode: 130\n",
            "The test reward for episode 130 is 24.0 with a standard deviation of 4.679743582719036.\n",
            "Episode: 140\n",
            "The test reward for episode 140 is 23.55 with a standard deviation of 4.811184885243967.\n",
            "Episode: 150\n",
            "The test reward for episode 150 is 31.8 with a standard deviation of 5.6000000000000005.\n",
            "Episode: 160\n",
            "The test reward for episode 160 is 46.4 with a standard deviation of 13.547693530634653.\n",
            "Episode: 170\n",
            "The test reward for episode 170 is 56.15 with a standard deviation of 20.018179237882748.\n",
            "Episode: 180\n",
            "The test reward for episode 180 is 112.7 with a standard deviation of 19.439907407186897.\n",
            "Episode: 190\n",
            "The test reward for episode 190 is 193.75 with a standard deviation of 12.848638060121392.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [00:32<02:08, 32.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0\n",
            "The test reward for episode 0 is 9.25 with a standard deviation of 0.698212002188447.\n",
            "Episode: 10\n",
            "The test reward for episode 10 is 9.3 with a standard deviation of 0.9539392014169455.\n",
            "Episode: 20\n",
            "The test reward for episode 20 is 9.4 with a standard deviation of 0.5830951894845301.\n",
            "Episode: 30\n",
            "The test reward for episode 30 is 9.35 with a standard deviation of 0.5722761571129799.\n",
            "Episode: 40\n",
            "The test reward for episode 40 is 9.45 with a standard deviation of 0.739932429347437.\n",
            "Episode: 50\n",
            "The test reward for episode 50 is 9.2 with a standard deviation of 0.812403840463596.\n",
            "Episode: 60\n",
            "The test reward for episode 60 is 9.45 with a standard deviation of 0.6689544080129827.\n",
            "Episode: 70\n",
            "The test reward for episode 70 is 9.1 with a standard deviation of 0.7.\n",
            "Episode: 80\n",
            "The test reward for episode 80 is 9.35 with a standard deviation of 0.7921489758877431.\n",
            "Episode: 90\n",
            "The test reward for episode 90 is 29.35 with a standard deviation of 3.9783790669065207.\n",
            "Episode: 100\n",
            "The test reward for episode 100 is 37.05 with a standard deviation of 4.0183952020676115.\n",
            "Episode: 110\n",
            "The test reward for episode 110 is 97.05 with a standard deviation of 32.493807102277195.\n",
            "Episode: 120\n",
            "The test reward for episode 120 is 148.65 with a standard deviation of 40.467610505192916.\n",
            "Episode: 130\n",
            "The test reward for episode 130 is 129.15 with a standard deviation of 44.7909310017106.\n",
            "Episode: 140\n",
            "The test reward for episode 140 is 169.8 with a standard deviation of 32.6275956821829.\n",
            "Episode: 150\n",
            "The test reward for episode 150 is 165.75 with a standard deviation of 30.04309404838323.\n",
            "Episode: 160\n",
            "The test reward for episode 160 is 168.85 with a standard deviation of 31.18537317397373.\n",
            "Episode: 170\n",
            "The test reward for episode 170 is 167.15 with a standard deviation of 27.59941122560407.\n",
            "Episode: 180\n",
            "The test reward for episode 180 is 143.45 with a standard deviation of 23.301233872908963.\n",
            "Episode: 190\n",
            "The test reward for episode 190 is 119.0 with a standard deviation of 25.78371579117331.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [01:13<01:52, 37.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0\n",
            "The test reward for episode 0 is 9.4 with a standard deviation of 0.66332495807108.\n",
            "Episode: 10\n",
            "The test reward for episode 10 is 9.4 with a standard deviation of 0.7348469228349533.\n",
            "Episode: 20\n",
            "The test reward for episode 20 is 9.45 with a standard deviation of 0.739932429347437.\n",
            "Episode: 30\n",
            "The test reward for episode 30 is 9.1 with a standard deviation of 0.8306623862918076.\n",
            "Episode: 40\n",
            "The test reward for episode 40 is 9.4 with a standard deviation of 0.66332495807108.\n",
            "Episode: 50\n",
            "The test reward for episode 50 is 9.25 with a standard deviation of 0.82915619758885.\n",
            "Episode: 60\n",
            "The test reward for episode 60 is 9.25 with a standard deviation of 0.698212002188447.\n",
            "Episode: 70\n",
            "The test reward for episode 70 is 9.55 with a standard deviation of 0.8046738469715539.\n",
            "Episode: 80\n",
            "The test reward for episode 80 is 9.55 with a standard deviation of 0.5894913061275798.\n",
            "Episode: 90\n",
            "The test reward for episode 90 is 9.25 with a standard deviation of 0.82915619758885.\n",
            "Episode: 100\n",
            "The test reward for episode 100 is 9.35 with a standard deviation of 0.6538348415311012.\n",
            "Episode: 110\n",
            "The test reward for episode 110 is 32.55 with a standard deviation of 5.78338136387356.\n",
            "Episode: 120\n",
            "The test reward for episode 120 is 33.7 with a standard deviation of 5.891519328662174.\n",
            "Episode: 130\n",
            "The test reward for episode 130 is 110.3 with a standard deviation of 60.71169574307738.\n",
            "Episode: 140\n",
            "The test reward for episode 140 is 137.4 with a standard deviation of 52.53322758026581.\n",
            "Episode: 150\n",
            "The test reward for episode 150 is 165.0 with a standard deviation of 44.267369472332554.\n",
            "Episode: 160\n",
            "The test reward for episode 160 is 151.15 with a standard deviation of 44.37034482624628.\n",
            "Episode: 170\n",
            "The test reward for episode 170 is 155.5 with a standard deviation of 40.47035952397754.\n",
            "Episode: 180\n",
            "The test reward for episode 180 is 153.6 with a standard deviation of 42.18459434438122.\n",
            "Episode: 190\n",
            "The test reward for episode 190 is 155.55 with a standard deviation of 28.72538076335978.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 3/5 [01:45<01:09, 34.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0\n",
            "The test reward for episode 0 is 9.25 with a standard deviation of 0.8874119674649424.\n",
            "Episode: 10\n",
            "The test reward for episode 10 is 9.55 with a standard deviation of 0.8046738469715539.\n",
            "Episode: 20\n",
            "The test reward for episode 20 is 9.25 with a standard deviation of 0.8874119674649424.\n",
            "Episode: 30\n",
            "The test reward for episode 30 is 9.6 with a standard deviation of 0.9165151389911681.\n",
            "Episode: 40\n",
            "The test reward for episode 40 is 9.25 with a standard deviation of 0.698212002188447.\n",
            "Episode: 50\n",
            "The test reward for episode 50 is 9.55 with a standard deviation of 0.864580823289529.\n",
            "Episode: 60\n",
            "The test reward for episode 60 is 9.15 with a standard deviation of 0.7921489758877431.\n",
            "Episode: 70\n",
            "The test reward for episode 70 is 9.5 with a standard deviation of 0.6708203932499369.\n",
            "Episode: 80\n",
            "The test reward for episode 80 is 9.3 with a standard deviation of 0.45825756949558405.\n",
            "Episode: 90\n",
            "The test reward for episode 90 is 30.4 with a standard deviation of 3.3075670817082456.\n",
            "Episode: 100\n",
            "The test reward for episode 100 is 36.4 with a standard deviation of 3.455430508634199.\n",
            "Episode: 110\n",
            "The test reward for episode 110 is 183.35 with a standard deviation of 16.891639944067006.\n",
            "Episode: 120\n",
            "The test reward for episode 120 is 197.1 with a standard deviation of 8.2516664983505.\n",
            "Episode: 130\n",
            "The test reward for episode 130 is 190.8 with a standard deviation of 20.98952119511067.\n",
            "Episode: 140\n",
            "The test reward for episode 140 is 181.3 with a standard deviation of 30.726373036855488.\n",
            "Episode: 150\n",
            "The test reward for episode 150 is 183.6 with a standard deviation of 29.928915783903697.\n",
            "Episode: 160\n",
            "The test reward for episode 160 is 173.65 with a standard deviation of 31.190182750346302.\n",
            "Episode: 170\n",
            "The test reward for episode 170 is 195.25 with a standard deviation of 19.37233852687899.\n",
            "Episode: 180\n",
            "The test reward for episode 180 is 185.95 with a standard deviation of 33.480554057542115.\n",
            "Episode: 190\n",
            "The test reward for episode 190 is 193.4 with a standard deviation of 16.37192719260625.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [02:40<00:43, 43.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0\n",
            "The test reward for episode 0 is 9.3 with a standard deviation of 0.6403124237432849.\n",
            "Episode: 10\n",
            "The test reward for episode 10 is 10.25 with a standard deviation of 1.6085707942145413.\n",
            "Episode: 20\n",
            "The test reward for episode 20 is 9.5 with a standard deviation of 0.5916079783099616.\n",
            "Episode: 30\n",
            "The test reward for episode 30 is 9.5 with a standard deviation of 0.6708203932499369.\n",
            "Episode: 40\n",
            "The test reward for episode 40 is 9.4 with a standard deviation of 0.7348469228349535.\n",
            "Episode: 50\n",
            "The test reward for episode 50 is 9.2 with a standard deviation of 0.7483314773547881.\n",
            "Episode: 60\n",
            "The test reward for episode 60 is 56.95 with a standard deviation of 15.755871921286998.\n",
            "Episode: 70\n",
            "The test reward for episode 70 is 136.55 with a standard deviation of 37.878060932418386.\n",
            "Episode: 80\n",
            "The test reward for episode 80 is 189.55 with a standard deviation of 17.451289350646846.\n",
            "Episode: 90\n",
            "The test reward for episode 90 is 187.65 with a standard deviation of 19.306151869287678.\n",
            "Episode: 100\n",
            "The test reward for episode 100 is 177.75 with a standard deviation of 31.06263832967187.\n",
            "Episode: 110\n",
            "The test reward for episode 110 is 181.95 with a standard deviation of 25.548923656389128.\n",
            "Episode: 120\n",
            "The test reward for episode 120 is 178.45 with a standard deviation of 24.822318586304544.\n",
            "Episode: 130\n",
            "The test reward for episode 130 is 165.5 with a standard deviation of 23.314158788169905.\n",
            "Episode: 140\n",
            "The test reward for episode 140 is 171.7 with a standard deviation of 21.760284924605195.\n",
            "Episode: 150\n",
            "The test reward for episode 150 is 148.75 with a standard deviation of 31.191144576626233.\n",
            "Episode: 160\n",
            "The test reward for episode 160 is 151.35 with a standard deviation of 30.929395403078928.\n",
            "Episode: 170\n",
            "The test reward for episode 170 is 161.75 with a standard deviation of 31.213578775910975.\n",
            "Episode: 180\n",
            "The test reward for episode 180 is 158.2 with a standard deviation of 32.46166970443757.\n",
            "Episode: 190\n",
            "The test reward for episode 190 is 139.05 with a standard deviation of 33.42973975369836.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [03:45<00:00, 45.10s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Avg. Return')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# set environment and training parameters\n",
        "env_name = 'CartPole-v0'\n",
        "num_episodes_train = 200\n",
        "num_episodes_test = 20\n",
        "learning_rate = 5e-4\n",
        "\n",
        "# create the environment\n",
        "env = gym.make(env_name)\n",
        "action_space_size = env.action_space.n\n",
        "state_space_size = 4\n",
        "\n",
        "# plot average performance of 5 trials\n",
        "num_seeds = 5\n",
        "l = num_episodes_train // 10\n",
        "res = np.zeros((num_seeds, l))\n",
        "gamma = 0.99\n",
        "\n",
        "# loop over multiple seeds\n",
        "for i in tqdm.tqdm(range(num_seeds)):\n",
        "    reward_means = []\n",
        "\n",
        "    # create an instance of the DQN_Agent class\n",
        "    agent = DQN_Agent(env_name, lr=learning_rate)\n",
        "\n",
        "    # training loop\n",
        "    for m in range(num_episodes_train):\n",
        "        agent.train()\n",
        "\n",
        "        # evaluate the agent every 10 episodes during training\n",
        "        if m % 10 == 0:\n",
        "            print(\"Episode: {}\".format(m))\n",
        "\n",
        "            # evaluate the agent's performance over 20 test episodes\n",
        "            G = np.zeros(num_episodes_test)\n",
        "            for k in range(num_episodes_test):\n",
        "                g = agent.test()\n",
        "                G[k] = g\n",
        "\n",
        "            reward_mean = G.mean()\n",
        "            reward_sd = G.std()\n",
        "            print(f\"The test reward for episode {m} is {reward_mean} with a standard deviation of {reward_sd}.\")\n",
        "            reward_means.append(reward_mean)\n",
        "\n",
        "    res[i] = np.array(reward_means)\n",
        "\n",
        "# plotting the average performance\n",
        "ks = np.arange(l) * 10\n",
        "avs = np.mean(res, axis=0)\n",
        "maxs = np.max(res, axis=0)\n",
        "mins = np.min(res, axis=0)\n",
        "\n",
        "plt.fill_between(ks, mins, maxs, alpha=0.1)\n",
        "plt.plot(ks, avs, '-o', markersize=1)\n",
        "\n",
        "plt.xlabel('Episode', fontsize=15)\n",
        "plt.ylabel('Avg. Return', fontsize=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ltPMiKYEz4QR"
      },
      "outputs": [],
      "source": [
        "# create CartPole environment\n",
        "env = gym.make('CartPole-v0', render_mode='rgb_array')\n",
        "state, _ = env.reset()\n",
        "frames = []  # list to store frames for video\n",
        "\n",
        "# run the environment for 20 steps\n",
        "for i in range(20):\n",
        "    frames.append(env.render())  # append current frame to the list\n",
        "\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        q_values = agent.policy_net.net(state)\n",
        "    action = agent.greedy_policy(q_values).detach().numpy()\n",
        "\n",
        "    # take the chosen action and observe the next state, reward, and termination status\n",
        "    state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "    # if the episode is terminated or truncated, reset the environment\n",
        "    if terminated or truncated:\n",
        "        state, _ = env.reset()\n",
        "\n",
        "env.close()  # close the environment after exploration\n",
        "\n",
        "# create a video from frames\n",
        "out = cv2.VideoWriter('cartpole_simulation_DQN.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 15, (frames[0].shape[1], frames[0].shape[0]))\n",
        "\n",
        "for frame in frames:\n",
        "    out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))  # convert RGB to BGR for OpenCV\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "SFP-oQ7X4_3M",
        "outputId": "2f835cfd-0e8e-4001-a8b9-84a104a6bb7a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<video controls  width=\"608\"  height=\"400\">\n",
              " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ],
            "text/plain": [
              "<IPython.core.display.Video object>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "video_path = '/cartpole_simulation_DQN.mp4'\n",
        "imageio.mimsave(video_path, frames, fps=30, macro_block_size=1)\n",
        "\n",
        "Video(video_path, embed=True, width=608, height=400)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
