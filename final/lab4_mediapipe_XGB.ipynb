{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqrad3M-jjGP",
        "outputId": "5d4f0cee-9acb-4eaa-c04b-7d1c68f9d578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# install mediapipe if it hasn't been installed already\n",
        "!pip install -q mediapipe\n",
        "!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\n",
        "\n",
        "# import the necessary libraries\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDQquHUlj7E2",
        "outputId": "f53c79a2-1e6e-42e0-8b12-1550cd174027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# if using colab, set to true [no other implementation of code currently]\n",
        "colab = True\n",
        "\n",
        "if colab:\n",
        "    # mount to google drive from colab\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # import synthetic data\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile('/content/drive/MyDrive/synthetic_asl_letters.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/data')\n",
        "        zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L2I4vQRx-pWE"
      },
      "outputs": [],
      "source": [
        "# process images with mediapipe hands overlay\n",
        "def process_images_with_mediapipe(base_dir, new_output_base_dir, splits=['train', 'test', 'valid']):\n",
        "    # initialize mediapipe hands module with specific settings\n",
        "    mp_hands = mp.solutions.hands\n",
        "    hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n",
        "    mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "    # process images for each split in the dataset ('train', 'test', 'valid')\n",
        "    for split in splits:\n",
        "        # define input and output directories for images\n",
        "        input_dir = os.path.join(base_dir, split, 'images')\n",
        "        output_dir = os.path.join(new_output_base_dir, split)\n",
        "        # create output directory if it doesn't exist\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # process each image in the input directory\n",
        "        for image_name in os.listdir(input_dir):\n",
        "            image_path = os.path.join(input_dir, image_name)\n",
        "            image = cv2.imread(image_path)\n",
        "            # continue to next image if current image is not found\n",
        "            if image is None:\n",
        "                continue\n",
        "            # convert image to rgb color space (required by mediapipe)\n",
        "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            # process the image using mediapipe hands\n",
        "            results = hands.process(image_rgb)\n",
        "            # if hand landmarks are detected, draw them on the image\n",
        "            if results.multi_hand_landmarks:\n",
        "                for hand_landmarks in results.multi_hand_landmarks:\n",
        "                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "            # save the processed image to the output directory\n",
        "            output_path = os.path.join(output_dir, image_name)\n",
        "            cv2.imwrite(output_path, image)\n",
        "\n",
        "    # release resources used by mediapipe hands\n",
        "    hands.close()\n",
        "\n",
        "# define base directory for input data and output directory for processed images\n",
        "base_dir = '/content/data/synthetic_asl_letters'\n",
        "new_output_base_dir = '/content/data/synthetic_asl_letters_mp'\n",
        "\n",
        "# call the function to start processing images\n",
        "process_images_with_mediapipe(base_dir, new_output_base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNyIEDgEaYFY"
      },
      "outputs": [],
      "source": [
        "# use code to download image file dataset\n",
        "!zip -r /content/data/synthetic_asl_letters_mp.zip /content/data/synthetic_asl_letters_mp\n",
        "# from google.colab import files\n",
        "# files.download(\"/content/data/synthetic_asl_letters_mp.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bTI5P54zimFC"
      },
      "outputs": [],
      "source": [
        "# create sorted dataset for mediapipe use\n",
        "\n",
        "# root path\n",
        "dataset_path = \"/content/data/synthetic_asl_letters\"\n",
        "\n",
        "# letter encoding mapping\n",
        "label_to_letter = {i: chr(65 + i) for i in range(26)}\n",
        "\n",
        "# create directories for a new sorted dataset structure if they don't already exist\n",
        "sorted_dataset_path = \"/content/sorted_synthetic_asl\"\n",
        "for split in [\"train\", \"test\", \"valid\"]:  # iterate over each data split\n",
        "    for letter in label_to_letter.values():  # iterate over each letter in the dataset\n",
        "        # ensure each directory for storing sorted images exists, creating them if necessary\n",
        "        os.makedirs(os.path.join(sorted_dataset_path, split, letter), exist_ok=True)\n",
        "\n",
        "# function to sort and move images based on their labels\n",
        "def sort_and_move_images(split):\n",
        "    # define the paths for images and labels within a specific split\n",
        "    images_path = os.path.join(dataset_path, split, \"images\")\n",
        "    labels_path = os.path.join(dataset_path, split, \"labels\")\n",
        "\n",
        "    # process each label file in the labels directory\n",
        "    for label_file in os.listdir(labels_path):\n",
        "        with open(os.path.join(labels_path, label_file), \"r\") as f:\n",
        "            # read the primary label (first number) from each label file\n",
        "            primary_label = int(f.readline().split()[0])\n",
        "            letter = label_to_letter[primary_label]  # map the numeric label to its corresponding letter\n",
        "\n",
        "        # construct the image filename by replacing the label file's extension\n",
        "        image_name = label_file.replace(\".txt\", \".jpg\")\n",
        "        source_path = os.path.join(images_path, image_name)  # source path of the image\n",
        "        destination_folder = os.path.join(sorted_dataset_path, split, letter)  # destination folder based on label\n",
        "\n",
        "        # copy the image from the source to the destination folder\n",
        "        shutil.copy(source_path, destination_folder)\n",
        "\n",
        "# perform for each split in data\n",
        "for split in [\"train\", \"test\", \"valid\"]:\n",
        "    sort_and_move_images(split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccHsDACKbldG"
      },
      "outputs": [],
      "source": [
        "# use code to download image file dataset\n",
        "!zip -r /content/sorted_synthetic_asl.zip /content/sorted_synthetic_asl\n",
        "# from google.colab import files\n",
        "# files.download(\"/content/sorted_synthetic_asl.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v0Ndi-f9pW8O"
      },
      "outputs": [],
      "source": [
        "# extract csv data from images\n",
        "def create_hand_landmarks_dataset(root_folder_path, dataset_split, csv_file_path):\n",
        "    # initialize mediapipe hand model\n",
        "    mp_hands = mp.solutions.hands.Hands(\n",
        "        static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n",
        "\n",
        "    # define path for the dataset split\n",
        "    split_folder_path = os.path.join(root_folder_path, dataset_split)\n",
        "    data = []\n",
        "\n",
        "    # iterate over each label directory in the split\n",
        "    for label in os.listdir(split_folder_path):\n",
        "        letter_folder_path = os.path.join(split_folder_path, label)\n",
        "        if os.path.isdir(letter_folder_path):\n",
        "            # process each image in the directory\n",
        "            for image_name in os.listdir(letter_folder_path):\n",
        "                if image_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    image_path = os.path.join(letter_folder_path, image_name)\n",
        "                    image = cv2.imread(image_path)\n",
        "                    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                    # apply mediapipe processing\n",
        "                    results = mp_hands.process(image_rgb)\n",
        "\n",
        "                    # extract hand landmarks if any are detected\n",
        "                    if results.multi_hand_landmarks:\n",
        "                        for hand_landmarks in results.multi_hand_landmarks:\n",
        "                            if len(hand_landmarks.landmark) == 21:\n",
        "                                row = {'image': image_name, 'label': label}\n",
        "                                for i, landmark in enumerate(hand_landmarks.landmark):\n",
        "                                    row[f'hand_{i}_x'] = landmark.x\n",
        "                                    row[f'hand_{i}_y'] = landmark.y\n",
        "                                    row[f'hand_{i}_z'] = landmark.z\n",
        "                                data.append(row)\n",
        "\n",
        "    # convert data into a dataframe\n",
        "    df = pd.DataFrame(data)\n",
        "    # save the dataframe to a csv file\n",
        "    df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "    # release resources used by mediapipe\n",
        "    mp_hands.close()\n",
        "\n",
        "# set the root folder path for the dataset\n",
        "root_folder_path = '/content/sorted_synthetic_asl'\n",
        "# define dataset splits to process\n",
        "dataset_splits = ['train', 'test', 'valid']\n",
        "# process each split and save to corresponding csv files\n",
        "for split in dataset_splits:\n",
        "    csv_file_path = f'hand_landmarks_{split}.csv'\n",
        "    create_hand_landmarks_dataset(root_folder_path, split, csv_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C_rBGoIwZNZ",
        "outputId": "219f324d-5f13-47e3-b273-fc87b8eaf3f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "since OS functions and nested for loops can be time intensive, this next section \n",
        "of code allows the ability to create/train/update the XGBoost model independently \n",
        "of running the previous code each time the notebook is opened for time purposes\n",
        "'''\n",
        "\n",
        "colab = True\n",
        "\n",
        "if colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    import zipfile\n",
        "\n",
        "    # import mediapipe hand landmark csv data for each split\n",
        "    with zipfile.ZipFile('/content/drive/MyDrive/hand_landmarks_test.csv.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content')\n",
        "        zip_ref.close()\n",
        "    with zipfile.ZipFile('/content/drive/MyDrive/hand_landmarks_train.csv.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content')\n",
        "        zip_ref.close()\n",
        "    with zipfile.ZipFile('/content/drive/MyDrive/hand_landmarks_valid.csv.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content')\n",
        "        zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7M2Du23zDb-r"
      },
      "outputs": [],
      "source": [
        "# load the dataset from csv files\n",
        "train = pd.read_csv('/content/hand_landmarks_train.csv', header=0)\n",
        "test = pd.read_csv('/content/hand_landmarks_test.csv', header=0)\n",
        "val = pd.read_csv('/content/hand_landmarks_valid.csv', header=0)\n",
        "\n",
        "# prepare feature matrices by dropping label and image columns\n",
        "X_train = train.drop(['label','image'], axis=1)\n",
        "y_train = train['label']\n",
        "\n",
        "X_test = test.drop(['label','image'], axis=1)\n",
        "y_test = test['label']\n",
        "\n",
        "X_val = val.drop(['label','image'], axis=1)\n",
        "y_val = val['label']\n",
        "\n",
        "# encode labels into integers\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "y_val = le.transform(y_val)\n",
        "\n",
        "# function for applying min-max scaling to a pandas dataframe row\n",
        "def min_max_scaling(row):\n",
        "    min_value = row.min()\n",
        "    max_value = row.max()\n",
        "    return (row - min_value) / (max_value - min_value)\n",
        "\n",
        "# apply normalization to feature matrices\n",
        "X_train_norm = X_train.apply(min_max_scaling, axis=1)\n",
        "X_test_norm = X_test.apply(min_max_scaling, axis=1)\n",
        "X_val_norm = X_val.apply(min_max_scaling, axis=1)\n",
        "\n",
        "# train a gradient boosting model using xgboost\n",
        "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train_norm, y_train)\n",
        "# predict the training data (usually you'd predict on test data to evaluate the model)\n",
        "predictions = gbm.predict(X_train_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B-E43qenz72s"
      },
      "outputs": [],
      "source": [
        "def f1_eval(y_pred, dtrain):\n",
        "    \"\"\"\n",
        "    Custom F1 evaluation metric for multi-class classification in XGBoost.\n",
        "    :param y_pred: The prediction of the model.\n",
        "    :param dtrain: XGBoost DMatrix with the true labels.\n",
        "    :return: Tuple (metric name, negative F1 score).\n",
        "    \"\"\"\n",
        "    y_true = dtrain.get_label()\n",
        "    # convert probabilities to the class with highest probability\n",
        "    preds = np.argmax(y_pred.reshape(len(np.unique(y_true)), -1), axis=0)\n",
        "    # calculate F1 score\n",
        "    f1 = f1_score(y_true, preds, average='macro')  # Use 'macro' to treat all classes equally\n",
        "    # return as 'negative' since XGBoost minimizes the loss\n",
        "    return 'negF1', -f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4RVYewJF4oJ_"
      },
      "outputs": [],
      "source": [
        "# create DMatrices for the train and test datasets since XGBoost is optimized for DMatrices\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HDXA6Xcf4rN_"
      },
      "outputs": [],
      "source": [
        "# define XGBoost parameters\n",
        "params = {\n",
        "    'objective': 'multi:softprob',  # use 'multi:softprob' for multi-class classification to get probabilities\n",
        "    'num_class': 26,  # number of unique classes\n",
        "    'eval_metric': 'mlogloss',  # multi-class logloss\n",
        "}\n",
        "\n",
        "evals_result = {}\n",
        "\n",
        "# train the model with custom evaluation metric\n",
        "bst = xgb.train(params, dtrain, num_boost_round=300, evals=[(dtest, 'test'),(dtrain, 'train')],\n",
        "                feval=f1_eval, evals_result=evals_result)\n",
        "\n",
        "display.clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "E2Q1meJEBcBx"
      },
      "outputs": [],
      "source": [
        "# create a dataframe with the results\n",
        "df_evals_result = pd.DataFrame({\n",
        "    'test_mlogloss': evals_result['test']['mlogloss'],\n",
        "    'test_negF1': evals_result['test']['negF1'],\n",
        "    'train_mlogloss': evals_result['train']['mlogloss'],\n",
        "    'train_negF1': evals_result['train']['negF1']\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LmTyTto9BsWZ"
      },
      "outputs": [],
      "source": [
        "# convert from negF1 with minimizing to normal F1 with maximizing\n",
        "df_evals_result['test_negF1'] = 1 + df_evals_result['test_negF1']\n",
        "df_evals_result['train_negF1'] = 1 + df_evals_result['train_negF1']\n",
        "\n",
        "# rename column to reflect change\n",
        "df_evals_result.rename(columns={'test_negF1': 'test_F1', 'train_negF1': 'train_F1'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7BGt_5t1FDu9"
      },
      "outputs": [],
      "source": [
        "# save to csv for analysis with other models, this was perfomed for both the normalized and unnormalized data\n",
        "df_evals_result.to_csv('xgb_mp_f1_loss.csv', index=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
